"""This script generates a local sqlite content table.

The columns of the table are

    id INTEGER PRIMARY KEY,
    text_ref_id INTEGER,
    text_content_id1 INTEGER,
    text_content_id2 INTEGER,
    text_type TEXT,
    content TEXT,

id is an autoincrementing primary key. text_ref_id is the id for the article
in indra_db's text_ref table. For a given article, indra_db can contain
one or more content types, either 'title', 'abstract', or 'fulltext'.
The content is stored in indra_db's text_content table. The local sqlite table
generated by this script stores only the best piece of content for each
text_ref_id. Due to a quirk of history, indra_db stores titles and abstracts
separately. For articles where the abstract is the best piece of content, the
local sqlite db will contain both the title and the abstract together. In this
case text_content_id1 will contain the text_content id for the abstract and
text_content_id2 will contain the text_content id for the title. When either
the title or fulltext is the best piece of content, text_content_id1 contains
the associated text_content id and text_content_id2 is NULL.

The text_type column contains one of 'title', 'abstract', or 'fulltext' and
should be self explanatory.

Each entry in the content column contains a json serialized list of paragraphs
for the associated content. When the best piece of content is the title, it
is considered as a single paragraph. When the best piece of content is the
abstract, the title and abstract are each considered as separate paragraphs.
Fulltexts are stored within indra_db in one of two XML formats from which it is
possible to parse out the paragraphs of the article.

This script is long running and requires considerable free disk space to
complete. In September of 2021, it took nearly 24 hours to run out completely
and required an estimated 500GB of free disk space to complete. The generated
output table was only 135GB in size, but a 210GB intermediate table is created
in the process and additional storage is needed for sqlite temporary tables
and the sqlite write-ahead-log which is used to speed up insertions.

The script takes a single positional argument, outpath, that specifies the
path to the folder where the temporary files, output sqlite db, and log file
is to be stored.

The script is designed so that it can be restarted in case of disruption.
In many cases it can simply be started again, though it may be necessary to
check the logs for signs of trouble.
"""


import argparse
from contextlib import closing
import json
import logging
import os
import sqlite3
from typing import Generator

from indra.literature.adeft_tools import universal_extract_paragraphs
from indra_db.util.helpers import unpack

from indra_db_lite.construction import get_row_count_postgres
from indra_db_lite.construction import get_row_count_sqlite
from indra_db_lite.construction import get_sqlite_tables
from indra_db_lite.construction import import_csv_into_sqlite
from indra_db_lite.construction import query_to_csv


def _extract_then_dump(hex_string: str) -> str:
    """Extract compressed content json serialized list of paragraphs."""
    return json.dumps(
        universal_extract_paragraphs(
            unpack(bytes.fromhex(hex_string))
        )
    )


def text_content_to_csv(outpath: str) -> None:
    """Dump indra databases text content table into a local csv file."""
    query = """
    SELECT
        id, text_ref_id, text_type, source, encode(content, 'hex')
    FROM
        text_content
    WHERE
        source NOT LIKE 'xdd%'
    """
    query_to_csv(query, outpath)


def ensure_text_content_table(sqlite_db_path: str) -> None:
    """Create the local text_content table if it doesn't exist."""
    query = \
        """-- Create text content table if it doesn't exist
        CREATE TABLE IF NOT EXISTS text_content (
            id INTEGER PRIMARY KEY,
            text_ref_id INTEGER NOT NULL,
            text_type TEXT NOT NULL,
            source TEXT NOT NULL,
            content TEXT NOT NULL
        );
        """
    with closing(sqlite3.connect(sqlite_db_path)) as conn:
        with closing(conn.cursor()) as cur:
            cur.execute(query)


def import_into_text_content_table(
        table_path: str,
        sqlite_db_path: str
) -> None:
    """Load rows into the local text_content table."""
    ensure_text_content_table(sqlite_db_path)
    import_csv_into_sqlite(table_path, 'text_content', sqlite_db_path)


def add_indices_to_text_content_table(sqlite_db_path: str) -> None:
    """Make key queries to local text_content table more efficient."""
    query1 = """--
    CREATE INDEX IF NOT EXISTS
        text_content_text_ref_idx
    ON
        text_content(text_ref_id)
    """
    query2 = """--
    CREATE INDEX IF NOT EXISTS
        text_content_text_ref_text_type_idx
    ON
        text_content(text_ref_id, text_type)
    """
    query3 = """--
    CREATE INDEX IF NOT EXISTS
        text_content_text_type_idx
    ON
        text_content(text_type)
    """
    with closing(sqlite3.connect(sqlite_db_path)) as conn:
        with closing(conn.cursor()) as cur:
            for query in query1, query2, query3:
                cur.execute(query)
        conn.commit()


def delete_content_for_which_fulltext_exists(sqlite_db_path: str) -> None:
    """Delete titles and abstracts for articles for which fulltext exists."""
    query = """--
    DELETE FROM text_content
    WHERE text_ref_id IN (
        SELECT text_ref_id FROM text_content WHERE text_type = 'fulltext'
    ) AND (text_type = 'abstract' OR text_type = 'title')
    """
    with closing(sqlite3.connect(sqlite_db_path)) as conn:
        with closing(conn.cursor()) as cur:
            cur.execute(query)
            conn.commit()


def delete_duplicate_fulltexts(sqlite_db_path: str) -> None:
    """Delete duplicate fulltexts, prioritizing duplicates by source."""
    query = """--
    DELETE FROM text_content
    WHERE
        text_type = 'fulltext' AND
        id NOT IN (
        SELECT id FROM (
            SELECT
                id, MIN(CASE source
                        WHEN 'pmc_oa' THEN 0
                        WHEN 'manuscripts' THEN 1
                        WHEN 'cord19_pmc_xml' THEN 2
                        WHEN 'elsevier' THEN 3
                        WHEN 'cord19_pdf' THEN 4
                        ELSE 5
                        END)
            FROM
                text_content
            GROUP BY text_ref_id)
    )
    """
    with closing(sqlite3.connect(sqlite_db_path)) as conn:
        with closing(conn.cursor()) as cur:
            cur.execute(query)
            conn.commit()


def delete_duplicate_abstracts(sqlite_db_path: str) -> None:
    """Delete duplicate abstracts, prioritizing duplicates by source."""
    query = """--
    DELETE FROM text_content
    WHERE
        text_type = 'abstract' AND
        id NOT IN (
        SELECT id FROM (
            SELECT
                id, MIN(CASE source
                        WHEN 'pubmed' THEN 0
                        WHEN 'cord19_abstract' THEN 1
                        ELSE 2
                        END)
            FROM
                text_content
            GROUP BY text_ref_id)
    )
    """
    with closing(sqlite3.connect(sqlite_db_path)) as conn:
        with closing(conn.cursor()) as cur:
            cur.execute(query)
            conn.commit()


def combine_abstracts_with_titles(sqlite_db_path: str) -> None:
    """Create new table with rows containing title and abstracts."""
    query = """--
    CREATE TABLE IF NOT EXISTS abstracts AS
    SELECT
        tc1.id AS tcid1, tc2.id AS tcid2,
        tc1.text_ref_id AS text_ref_id, tc1.text_type AS text_type,
        tc1.source AS source, tc2.content AS title, tc1.content AS abstract
    FROM
        text_content tc1
    INNER JOIN
        text_content tc2
    ON
        tc1.text_type = 'abstract' AND
        tc2.text_type = 'title' AND
        tc1.text_ref_id = tc2.text_ref_id
    """
    with closing(sqlite3.connect(sqlite_db_path)) as conn:
        with closing(conn.cursor()) as cur:
            cur.execute(query)
        conn.commit()


def add_index_to_abstracts_table(sqlite_db_path: str) -> None:
    """Make queries to abstracts table more efficient."""
    query = """--
    CREATE INDEX IF NOT EXISTS
        abstracts_text_ref_idx
    ON
        abstracts(text_ref_id)
    """
    with closing(sqlite3.connect(sqlite_db_path)) as conn:
        with closing(conn.cursor()) as cur:
            cur.execute(query)
        conn.commit()


def delete_titles_for_which_abstracts_exist(sqlite_db_path: str) -> None:
    """Deletes titles which are known not to be best content."""
    query = """--
    DELETE FROM text_content
    WHERE text_ref_id IN (
        SELECT text_ref_id FROM text_content WHERE text_type = 'abstract'
    ) AND text_type = 'title'
    """
    with closing(sqlite3.connect(sqlite_db_path)) as conn:
        with closing(conn.cursor()) as cur:
            cur.execute(query)
            conn.commit()


def ensure_best_content_table(sqlite_db_path: str) -> None:
    """Create the best content table if it does not yet exist."""
    query = """--
    CREATE TABLE IF NOT EXISTS best_content (
        id INTEGER PRIMARY KEY,
        text_ref_id INTEGER,
        text_content_id1 INTEGER,
        text_content_id2 INTEGER,
        text_type TEXT,
        content TEXT,
        UNIQUE(text_content_id1),
        UNIQUE(text_ref_id))
    """
    with closing(sqlite3.connect(sqlite_db_path)) as conn:
        with closing(conn.cursor()) as cur:
            cur.execute(query)
        conn.commit()


def abstracts_generator(
        sqlite_db_path: str,
        batch_size: int = 10000,
        starting_text_ref_id: int = 0,
) -> Generator[list, None, None]:
    with closing(sqlite3.connect(sqlite_db_path)) as conn:
        with closing(conn.cursor()) as cur:
            query = """--
            SELECT * FROM abstracts
            WHERE
                text_ref_id >= ?
            ORDER BY
                text_ref_id
            """
            cur.execute(query, (starting_text_ref_id, ))
            while True:
                result = cur.fetchmany(batch_size)
                if not result:
                    break
                yield [
                    [
                        None,
                        text_ref_id,
                        tcid1,
                        tcid2,
                        text_type,
                        json.dumps(
                            [
                                unpack(bytes.fromhex(title)),
                                unpack(bytes.fromhex(abstract)),
                            ]
                        )
                    ]
                    for (
                            tcid1,
                            tcid2,
                            text_ref_id,
                            text_type,
                            _,
                            title,
                            abstract,
                    ) in result
                ]


def fulltexts_and_titles_generator(
        sqlite_db_path: str,
        batch_size: int = 10000,
        starting_text_ref_id: int = 0,
) -> Generator[list, None, None]:
    with closing(sqlite3.connect(sqlite_db_path)) as conn:
        with closing(conn.cursor()) as cur:
            query = """--
            SELECT * FROM text_content
            WHERE
                text_ref_id >= ? AND text_type != 'abstract'
            ORDER BY
                text_ref_id
            """
            cur.execute(query, (starting_text_ref_id, ))
            while True:
                result = cur.fetchmany(batch_size)
                if not result:
                    break
                yield [
                    [
                        None,
                        text_ref_id,
                        tcid,
                        None,
                        text_type,
                        _extract_then_dump(content),
                    ]
                    for (
                            tcid,
                            text_ref_id,
                            text_type,
                            _,
                            content
                    ) in result
                ]


def load_best_content_table(
        sqlite_db_to_path: str,
        row_generator: Generator[list, None, None],
) -> None:
    """Load best content table with rows from generator."""
    insertion_query = """--
    INSERT OR IGNORE INTO
        best_content (id, text_ref_id,
                      text_content_id1, text_content_id2, text_type, content)
    VALUES (?, ?, ?, ?, ?, ?);
    """
    with closing(sqlite3.connect(sqlite_db_to_path)) as conn:
        with closing(conn.cursor()) as cur:
            ensure_best_content_table(sqlite_db_to_path)
            cur.execute('PRAGMA journal_mode = WAL')
            cur.execute('PRAGMA synchronous = NORMAL')
            for block, row_list in enumerate(row_generator):
                logger.info(
                    f"Inserting {len(row_list)} rows from block {block}"
                )
                cur.executemany(insertion_query, row_list)
                conn.commit()
            cur.execute('PRAGMA journal_mode = DELETE')


def add_index_to_best_content_table(sqlite_db_path: str) -> None:
    """Make queries to best content table more efficient."""
    query = """--
    CREATE INDEX IF NOT EXISTS
        best_content_text_ref_id_idx
    ON
        best_content(text_ref_id)
    """
    with closing(sqlite3.connect(sqlite_db_path)) as conn:
        with closing(conn.cursor()) as cur:
            cur.execute(query)
        conn.commit()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("outpath")
    args = parser.parse_args()
    outpath = args.outpath
    temp_db_path = os.path.join(outpath, 'text_content.db')
    best_content_db_path = os.path.join(outpath, 'content.db')
    csv_path = os.path.join(outpath, 'text_content.csv')
    logging.basicConfig(
        filename=os.path.join(outpath, 'best_content.log'),
        filemode='a',
        format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
        datefmt='%m-%d %H:%M',
        level=logging.DEBUG,
        force=True,
    )
    logger = logging.getLogger(__name__)

    logger.info('Collecting number of rows in indra_db text_content table.')
    num_rows_text_content = get_row_count_postgres('text_content')
    logger.info(f"There are {num_rows_text_content} rows.")
    if not os.path.exists(temp_db_path):
        logger.info('loading text content to csv file')
        text_content_to_csv(csv_path)
        logger.info('importing text content into sqlite db')
        import_into_text_content_table(csv_path, temp_db_path)
        num_rows_text_content_local = get_row_count_sqlite(
            'text_content', temp_db_path
        )
        try:
            assert num_rows_text_content == num_rows_text_content_local
        except AssertionError:
            logger.exception(
                "Dump from indra_db text_content table failed."
                f" There are {num_rows_text_content} in indra_db's"
                f" text_content table, but only {num_rows_text_content_local}"
                " in the local dump."
            )
        logger.info(
            f"All {num_rows_text_content_local} rows have been loaded"
            " into the local db."
        )
        logger.info('removing text content csv')
        os.remove(csv_path)
    logger.info('adding indices to text content table of sqlite db')
    add_indices_to_text_content_table(temp_db_path)
    if 'abstracts' not in get_sqlite_tables(temp_db_path):
        logger.info('Removing lesser content for which fulltexts exist.')
        delete_content_for_which_fulltext_exists(temp_db_path)
        logger.info('Removing duplicate fulltexts')
        delete_duplicate_fulltexts(temp_db_path)
        logger.info('Removing duplicate abstracts')
        delete_duplicate_abstracts(temp_db_path)
        logger.info('combining abstracts with titles into new table')
        combine_abstracts_with_titles(temp_db_path)
        logger.info('deleting abstracts from text content table')
        logger.info('deleting remaining titles that aren\'t best content')
        delete_titles_for_which_abstracts_exist(temp_db_path)
    add_index_to_abstracts_table(temp_db_path)
    num_abstracts = get_row_count_sqlite('abstracts', temp_db_path)
    num_content = get_row_count_sqlite('text_content', temp_db_path)
    logger.info(
        f'There are {num_abstracts} abstracts out of {num_content}'
        ' total pieces of best content.'
    )
    if 'best_content' in get_sqlite_tables(best_content_db_path):
        logger.info(
            'Best content table exists. Determining what has already'
            ' been inserted.'
        )
        with closing(sqlite3.connect(best_content_db_path)) as conn:
            with closing(conn.cursor()) as cur:
                latest_abstract_query = """--
                SELECT
                    MAX(text_ref_id)
                FROM
                    best_content
                WHERE
                    text_type = 'abstract'
                """
                latest_abstract = cur.execute(latest_abstract_query).\
                    fetchone()[0]
                latest_non_abstract_query = """--
                SELECT
                    MAX(text_ref_id)
                FROM
                    best_content
                WHERE
                    text_type != 'abstract'
                """
                latest_non_abstract = cur.execute(latest_non_abstract_query).\
                    fetchone()[0]
                if not latest_non_abstract:
                    latest_non_abstract = 0
    else:
        latest_abstract = 0
        latest_non_abstract = 0
    logger.info('loading abtracts into best content table')
    gen = abstracts_generator(
        temp_db_path,
        batch_size=5000000,
        starting_text_ref_id=latest_abstract,
    )
    load_best_content_table(best_content_db_path, gen)
    num_abstracts_best_content = get_row_count_sqlite(
        'best_content',
        best_content_db_path,
    )
    try:
        assert num_abstracts == num_abstracts_best_content
    except AssertionError:
        logger.exception(
            "Loading of abstracts into best_content failed. Only"
            f" {num_abstracts_best_content} have been loaded."
        )
        raise
    logger.info(
        f"All {num_abstracts_best_content} abstracts loaded successfully."
    )
    logger.info('loading fulltexts and titles into best content table')
    gen = fulltexts_and_titles_generator(
        temp_db_path,
        batch_size=100000,
        starting_text_ref_id=latest_non_abstract + 1,
    )
    load_best_content_table(best_content_db_path, gen)
    num_rows_best_content = get_row_count_sqlite(
        'best_content', best_content_db_path
    )
    logger.info(
        f'{num_rows_best_content} have been inserted out of'
        f' {num_content} possible rows.'
    )
    logger.info("Successfully loaded best content table.")
    logger.info("Adding index to best content table.")
    add_index_to_best_content_table(best_content_db_path)
    if num_rows_best_content == num_content:
        logger.info('Removing temporary text content db')
        os.remove(temp_db_path)
    else:
        logger.info(
            f'Temporary db at {temp_db_path} has not been removed since'
            f' there is a discrepancy between the number rows in the number'
            'of best_content rows in the sqlite database'
            '{best_content_db_path} and the number in the temporary db.'
        )
